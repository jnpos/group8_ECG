{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jnpos/group8_ECG/blob/Classifier/ECG_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/jveenland/tm10007_ml.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8iRqJzPriLC",
        "outputId": "99ab56de-6c6a-43d4-fdf3-96d99ecbbb49"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'tm10007_ml'...\n",
            "remote: Enumerating objects: 83, done.\u001b[K\n",
            "remote: Total 83 (delta 0), reused 0 (delta 0), pack-reused 83\u001b[K\n",
            "Unpacking objects: 100% (83/83), 67.99 MiB | 5.63 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# General packages\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets as ds\n",
        "from sklearn import metrics\n",
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Metrics\n",
        "#from sklearn.metrics import confusion_matrix\n",
        "#from sklearn.metrics import mean_absolute_error\n",
        "# from sklearn.metrics import r2_score\n",
        "\n",
        "# Classifiers\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import model_selection\n",
        "from sklearn import preprocessing\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import uniform\n",
        "from sklearn.metrics import f1_score, make_scorer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import fbeta_score"
      ],
      "metadata": {
        "id": "047Gr7IytEXy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYsZqFpNnCHH",
        "outputId": "45f783cc-b9f0-4c16-e613-ed410878234b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of samples: 827\n",
            "The number of columns: 9001\n"
          ]
        }
      ],
      "source": [
        "with zipfile.ZipFile('/content/tm10007_ml/ecg/ecg_data.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/tm10007_ml/ecg')\n",
        "\n",
        "data = pd.read_csv('/content/tm10007_ml/ecg/ecg_data.csv', index_col=0)\n",
        "\n",
        "print(f'The number of samples: {len(data.index)}')\n",
        "print(f'The number of columns: {len(data.columns)}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split input, output, test and train data:**"
      ],
      "metadata": {
        "id": "EhdR607ik017"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find column with label \n",
        "bool_cols = [col for col in data \n",
        "             if np.isin(data[col].dropna().unique(), [0, 1]).all()]\n",
        "loc_label = data.columns.get_loc('label')\n",
        "\n",
        "# Determine data and output \n",
        "y = data['label']\n",
        "x = pd.DataFrame()\n",
        "x = data.drop(data.columns[loc_label],axis=1)\n"
      ],
      "metadata": {
        "id": "q7ySkT_1rge5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete patients (samples) with missing data\n",
        "deleted_patients = []\n",
        "zero_samples = []\n",
        "num_zeros = []\n",
        "zero_counts = []\n",
        "eenvariabele = []\n",
        "print(len(x))\n",
        "for i in range(len(x)):\n",
        "  num_zeros = (x.iloc[i] == 0).sum()\n",
        "  if (num_zeros/750 >= 1) & (num_zeros%750 == 0):                   # met deze regel controleer je of het aantal 0'en overeenkomt met het aantal frequentie features dat in 1 lead zit\n",
        "    deleted_patients.append(i)\n",
        "    zero_samples.append(\"Sample_\" + str(i))\n",
        "    zero_counts.append(num_zeros)\n",
        "\n",
        "x_dropped = x.drop(deleted_patients)\n",
        "x_dropped = x_dropped.reset_index(drop=True)\n",
        "y_dropped = y.drop(deleted_patients)\n",
        "y_dropped = y_dropped.reset_index(drop=True)\n",
        "table = pd.DataFrame({'Sample ID': zero_samples, 'Zero count': zero_counts})\n",
        "\n",
        "print(table)\n",
        "print(f'The number of samples: {len(x_dropped.index)}')\n",
        "print(f'The number of columns: {len(x_dropped.columns)}')"
      ],
      "metadata": {
        "id": "rFwm6zGp31Sa",
        "outputId": "e9dafdae-6590-44f8-ac43-5406aae73e48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "827\n",
            "     Sample ID  Zero count\n",
            "0   Sample_177         750\n",
            "1   Sample_251         750\n",
            "2   Sample_269         750\n",
            "3   Sample_321         750\n",
            "4   Sample_323         750\n",
            "5   Sample_385         750\n",
            "6   Sample_434         750\n",
            "7   Sample_446         750\n",
            "8   Sample_537         750\n",
            "9   Sample_542         750\n",
            "10  Sample_575         750\n",
            "11  Sample_601         750\n",
            "12  Sample_784         750\n",
            "13  Sample_790         750\n",
            "The number of samples: 813\n",
            "The number of columns: 9000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split test and trainingsdata \n",
        "x_train, x_test, y_train, y_test = model_selection.train_test_split(x_dropped, y_dropped,test_size=0.25,random_state=0,stratify=y_dropped)"
      ],
      "metadata": {
        "id": "36evIYp834Wz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scaling:**"
      ],
      "metadata": {
        "id": "ubWqftf7jVNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SCALEN\n",
        "scaler = preprocessing.RobustScaler()\n",
        "scaler.fit(x_train)\n",
        "x_train = pd.DataFrame(scaler.transform(x_train))\n",
        "\n",
        "# Niet fitten op test, alleen toepassen\n"
      ],
      "metadata": {
        "id": "pTeKfmHRI8I4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking if data is normally distributed:**\n"
      ],
      "metadata": {
        "id": "R0A0uDkswrg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if data is normally distributed  \n",
        "\n",
        "from scipy.stats import shapiro  \n",
        "shapiro(x_train)\n",
        "\n",
        "amount_normallydistributed = 0\n",
        "for column in x_train.columns:\n",
        "  result = shapiro(x_train[column])\n",
        "  normallydistributed = result.pvalue > 0.05\n",
        "  amount_normallydistributed += normallydistributed \n",
        "  \n",
        "print(amount_normallydistributed, \"features are normally distributed features\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-T7l12igocL",
        "outputId": "05a0f0ae-afca-460d-e680-ba6b2f2dc742"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/scipy/stats/_morestats.py:1816: UserWarning: p-value may not be accurate for N > 5000.\n",
            "  warnings.warn(\"p-value may not be accurate for N > 5000.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 features are normally distributed features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature selection:**"
      ],
      "metadata": {
        "id": "Wejl1fh6jcmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature selection\n",
        "\n",
        "# Univariate & f_classif\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "selector = SelectKBest(f_classif,k=\"all\")\n",
        "selector.fit(x_train,y_train)\n",
        "scores = -np.log10(selector.pvalues_)\n",
        "scores /= scores.max()\n",
        "\n",
        "dataframe = pd.DataFrame()\n",
        "dataframe = selector.pvalues_\n",
        "\n",
        "print(len(selector.pvalues_[selector.pvalues_<0.05]))\n",
        "\n",
        "fs = SelectKBest(score_func=f_classif, k=len(selector.pvalues_[selector.pvalues_<0.05]))\n",
        "X_univariate = fs.fit_transform(x_train, y_train)\n",
        "print(f'result univariate: {X_univariate.shape}')\n",
        "\n",
        "# Elastic net\n",
        "from sklearn.linear_model import ElasticNet, Lasso, ElasticNetCV\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "#for i in np.arange(0.02,1,0.02):\n",
        "#regr = ElasticNetCV(cv=5, random_state=0)\n",
        "#regr.fit(x_train, y_train)\n",
        "#print(regr.alpha_)\n",
        "#print(regr.intercept_)\n",
        "regr_alpha_ = 0.3858647907662583\n",
        "\n",
        "ENreg = ElasticNet(alpha=regr_alpha_, l1_ratio=0.5).fit(x_train,y_train)\n",
        "model_ENreg = SelectFromModel(ENreg, prefit=True)\n",
        "x_ENreg = model_ENreg.transform(x_train)\n",
        "#print(f'result Elastic Net: {x_ENreg.shape}, met als i:{i}')\n",
        "print(f'result Elastic Net: {x_ENreg.shape}')\n"
      ],
      "metadata": {
        "id": "mFaLdfI9i5eP",
        "outputId": "7f223bd2-81fc-42db-f661-659f0610715b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "855\n",
            "result univariate: (609, 855)\n",
            "result Elastic Net: (609, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PCA:**"
      ],
      "metadata": {
        "id": "zWgB_O64jZVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA\n",
        "X_univariate = pd.DataFrame(X_univariate)\n",
        "n_samples = len(X_univariate)\n",
        "n_features = len(X_univariate.columns)\n",
        "n_features = min((n_samples//2), n_features)\n",
        "print(n_features)\n",
        "\n",
        "p = PCA(n_components=n_features)\n",
        "p = p.fit(X_univariate)\n",
        "x_pca = p.transform(X_univariate)\n",
        "\n",
        "print(f'result pca: {x_pca.shape}')"
      ],
      "metadata": {
        "id": "nUTJsxovisgU",
        "outputId": "2dc1f8b2-ecd2-4978-82c1-a5776f592665",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "304\n",
            "result pca: (609, 304)\n",
            "304\n",
            "result pca: (609, 304)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimize hyperparameters of all classifiers:**"
      ],
      "metadata": {
        "id": "cKBOYqkscOwn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSVM"
      ],
      "metadata": {
        "id": "7niOsrJQcUCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l_svm = LinearSVC()\n",
        "#logistic = LogisticRegression(solver='saga', tol=1e-2, max_iter=200,\n",
        "                              #random_state=0)\n",
        "distributions = dict(penalty=['l2', 'l1'],\n",
        "                     loss = ['hinge','squared_hinge'],                                       \n",
        "                     dual=[False, False],\n",
        "                     tol = np.geomspace(1e-7, 1e2, num=10),\n",
        "                     C = np.geomspace(1e-5, 1e5, num=11),\n",
        "                     fit_intercept=[True, True],\n",
        "                     intercept_scaling = range(1,10),\n",
        "                     class_weight = ['balanced','balanced'])\n",
        "#y_pred = clf.predict(x_train)\n",
        "#l_svm.fit(x_train, y_train)\n",
        "\n",
        "\n",
        "#y_pred = l_svm.fit(x_train, y_train).predict(x_train)\n",
        "\n",
        "clf = RandomizedSearchCV(l_svm, distributions, random_state=0, error_score=0.0, scoring=make_scorer(fbeta_score, beta=2)) #dual = False als n_samples > n_features \n",
        "search = clf.fit(x_ENreg, y_train)\n",
        "print(search.best_params_)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLEYc6rhe8Kp",
        "outputId": "f4936903-cd41-47c4-ce28-b15b8d03f7b9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'tol': 100.0, 'penalty': 'l1', 'loss': 'squared_hinge', 'intercept_scaling': 9, 'fit_intercept': True, 'dual': False, 'class_weight': 'balanced', 'C': 0.01}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "30 fits failed out of a total of 50.\n",
            "The score on these train-test partitions for these parameters will be set to 0.0.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
            "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
            "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
            "    raise ValueError(\n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "25 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
            "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
            "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
            "    raise ValueError(\n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'tol': 0.01, 'penalty': 'l2', 'loss': 'squared_hinge', 'intercept_scaling': 6, 'fit_intercept': True, 'dual': False, 'class_weight': 'balanced', 'C': 0.01}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "30 fits failed out of a total of 50.\n",
            "The score on these train-test partitions for these parameters will be set to 0.0.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
            "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
            "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
            "    raise ValueError(\n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "25 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
            "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
            "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
            "    raise ValueError(\n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DT"
      ],
      "metadata": {
        "id": "XpPbWSP6cYse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import tree\n",
        "import numpy as np\n",
        "\n",
        "tree = tree.DecisionTreeClassifier()\n",
        "\n",
        "distributions = dict(criterion =['gini','entropy','log_loss'],\n",
        "                     splitter=['best','random'],\n",
        "                     min_samples_split=range(2,10),\n",
        "                     min_samples_leaf=range(1,10),\n",
        "                     min_weight_fraction_leaf = np.linspace(0, 0.5, 25),\n",
        "                     max_features=['sqrt','log2',None],\n",
        "                     class_weight=['balanced','balanced'])\n",
        "\n",
        "clf = RandomizedSearchCV(tree, distributions, random_state=0, error_score=0.0, scoring=make_scorer(fbeta_score, beta=2)) \n",
        "search = clf.fit(x_ENreg, y_train)\n",
        "print(search.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hz7U2TsDbQ-4",
        "outputId": "a2361b73-9de0-4a4d-8afa-dab3260ad6ac"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'splitter': 'best', 'min_weight_fraction_leaf': 0.16666666666666666, 'min_samples_split': 2, 'min_samples_leaf': 6, 'max_features': None, 'criterion': 'entropy', 'class_weight': 'balanced'}\n",
            "{'splitter': 'random', 'min_weight_fraction_leaf': 0.020833333333333332, 'min_samples_split': 2, 'min_samples_leaf': 5, 'max_features': None, 'criterion': 'log_loss', 'class_weight': 'balanced'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVC"
      ],
      "metadata": {
        "id": "7JMTufoxh7BH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "\n",
        "svm = svm.SVC()\n",
        "\n",
        "distributions = dict(C = np.geomspace(1e-5, 1e5, num=11),\n",
        "                     kernel=['linear','poly','rbf','sigmoid'],\n",
        "                     degree = range(0,5),\n",
        "                     gamma=['scale','auto'],\n",
        "                     coef0 = range(-5,5),\n",
        "                     shrinking = [True,False],\n",
        "                     probability = [True, False],\n",
        "                     tol = np.geomspace(1e-7, 1e2, num=10),\n",
        "                     cache_size=range(10,400),\n",
        "                     class_weight = ['balanced','balanced'])\n",
        "\n",
        "clf = RandomizedSearchCV(svm, distributions, random_state=0, error_score=0.0, scoring=make_scorer(fbeta_score, beta=2)) #dual = False als n_samples > n_features \n",
        "\n",
        "search = clf.fit(x_ENreg, y_train)\n",
        "print(search.best_params_)"
      ],
      "metadata": {
        "id": "aDQtky5Hh5Yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RF for X_univariate:"
      ],
      "metadata": {
        "id": "xlQBNVTonxSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "RF = RandomForestClassifier()\n",
        "\n",
        "distributions = dict(n_estimators = range(5,1000,5),\n",
        "                     criterion =['gini','entropy','log_loss'],\n",
        "                     min_samples_split=range(2,10),\n",
        "                     min_samples_leaf=range(1,10),\n",
        "                     min_weight_fraction_leaf = np.linspace(0, 0.5, 25),\n",
        "                     max_features=['sqrt','log2',None],\n",
        "                     bootstrap=[True,False],\n",
        "                     oob_score=[True,False],\n",
        "                     warm_start=[True,False],\n",
        "                     class_weight=['balanced','balanced'])\n",
        "\n",
        "clf = RandomizedSearchCV(RF, distributions, random_state=0, error_score=0.0, scoring=make_scorer(fbeta_score, beta=2)) \n",
        "search = clf.fit(X_univariate, y_train)\n",
        "print(search.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Be41HWQNny5q",
        "outputId": "decdffe3-3fed-4687-d7c7-92ca81283cd4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'warm_start': True, 'oob_score': True, 'n_estimators': 555, 'min_weight_fraction_leaf': 0.1875, 'min_samples_split': 4, 'min_samples_leaf': 8, 'max_features': None, 'criterion': 'entropy', 'class_weight': 'balanced', 'bootstrap': True}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RF for x_ENreg:"
      ],
      "metadata": {
        "id": "nqQnhgtCMrg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "RF = RandomForestClassifier()\n",
        "\n",
        "distributions = dict(n_estimators = range(5,1000,5),\n",
        "                     criterion =['gini','entropy','log_loss'],\n",
        "                     min_samples_split=range(2,10),\n",
        "                     min_samples_leaf=range(1,10),\n",
        "                     min_weight_fraction_leaf = np.linspace(0, 0.5, 25),\n",
        "                     max_features=['sqrt','log2',None],\n",
        "                     bootstrap=[True,False],\n",
        "                     oob_score=[True,False],\n",
        "                     warm_start=[True,False],\n",
        "                     class_weight=['balanced','balanced'])\n",
        "\n",
        "clf = RandomizedSearchCV(RF, distributions, random_state=0, error_score=0.0, scoring=make_scorer(fbeta_score, beta=2)) \n",
        "search = clf.fit(x_ENreg, y_train)\n",
        "print(search.best_params_)"
      ],
      "metadata": {
        "id": "Fj4QWiPrMtIu",
        "outputId": "325ddb0a-2edb-4b03-e92a-01417a963ffa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'warm_start': True, 'oob_score': True, 'n_estimators': 795, 'min_weight_fraction_leaf': 0.14583333333333331, 'min_samples_split': 9, 'min_samples_leaf': 5, 'max_features': 'log2', 'criterion': 'gini', 'class_weight': 'balanced', 'bootstrap': True}\n"
          ]
        }
      ]
    }
  ]
}