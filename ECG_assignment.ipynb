{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jnpos/group8_ECG/blob/Classifier/ECG_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/jveenland/tm10007_ml.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8iRqJzPriLC",
        "outputId": "cd95176d-2adc-4ed8-e835-3bf2687c4892"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'tm10007_ml' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# General packages\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets as ds\n",
        "from sklearn import metrics\n",
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Metrics\n",
        "#from sklearn.metrics import confusion_matrix\n",
        "#from sklearn.metrics import mean_absolute_error\n",
        "# from sklearn.metrics import r2_score\n",
        "\n",
        "# Classifiers\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import model_selection\n",
        "from sklearn import preprocessing"
      ],
      "metadata": {
        "id": "047Gr7IytEXy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYsZqFpNnCHH",
        "outputId": "e1ea60b1-6f4d-4bca-b31b-d9a865eb2e3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of samples: 827\n",
            "The number of columns: 9001\n"
          ]
        }
      ],
      "source": [
        "with zipfile.ZipFile('/content/tm10007_ml/ecg/ecg_data.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/tm10007_ml/ecg')\n",
        "\n",
        "data = pd.read_csv('/content/tm10007_ml/ecg/ecg_data.csv', index_col=0)\n",
        "\n",
        "print(f'The number of samples: {len(data.index)}')\n",
        "print(f'The number of columns: {len(data.columns)}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split input, output, test and train data:**"
      ],
      "metadata": {
        "id": "EhdR607ik017"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find column with label \n",
        "bool_cols = [col for col in data \n",
        "             if np.isin(data[col].dropna().unique(), [0, 1]).all()]\n",
        "loc_label = data.columns.get_loc('label')\n",
        "\n",
        "# Determine data and output \n",
        "y = data['label']\n",
        "x = pd.DataFrame()\n",
        "x = data.drop(data.columns[loc_label],axis=1)\n",
        "\n",
        "# Split test and trainingsdata \n",
        "x_train, x_test, y_train, y_test = model_selection.train_test_split(x, y,test_size=0.25,random_state=0,stratify=y)\n",
        "\n"
      ],
      "metadata": {
        "id": "q7ySkT_1rge5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scaling:**"
      ],
      "metadata": {
        "id": "ubWqftf7jVNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SCALEN\n",
        "scaler = preprocessing.RobustScaler()\n",
        "scaler.fit(x_train)\n",
        "x_train = pd.DataFrame(scaler.transform(x_train))\n",
        "\n",
        "# Niet fitten op test, alleen toepassen\n"
      ],
      "metadata": {
        "id": "pTeKfmHRI8I4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking if data is normally distributed:**\n"
      ],
      "metadata": {
        "id": "R0A0uDkswrg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if data is normally distributed  \n",
        "\n",
        "from scipy.stats import shapiro  \n",
        "shapiro(x_train)\n",
        "\n",
        "amount_normallydistributed = 0\n",
        "for column in x_train.columns:\n",
        "  result = shapiro(x_train[column])\n",
        "  normallydistributed = result.pvalue > 0.05\n",
        "  amount_normallydistributed += normallydistributed \n",
        "  \n",
        "print(amount_normallydistributed, \"features are normally distributed features\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-T7l12igocL",
        "outputId": "1d29c2e1-c468-4508-ba28-76e216f8176e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/scipy/stats/_morestats.py:1816: UserWarning: p-value may not be accurate for N > 5000.\n",
            "  warnings.warn(\"p-value may not be accurate for N > 5000.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 features are normally distributed features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PCA:**"
      ],
      "metadata": {
        "id": "zWgB_O64jZVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA\n",
        "n_samples = len(x_train)\n",
        "n_features = len(x_train.columns)\n",
        "n_features = min((n_samples-1), n_features)\n",
        "print(n_features)\n",
        "\n",
        "p = PCA(n_components=n_features)\n",
        "p = p.fit(x_train)\n",
        "x_pca = p.transform(x_train)\n",
        "\n",
        "print(f'result pca: {x_pca.shape}')"
      ],
      "metadata": {
        "id": "nUTJsxovisgU",
        "outputId": "bb8ec5ce-f842-426c-8830-555397b58549",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "619\n",
            "result pca: (620, 619)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature selection:**"
      ],
      "metadata": {
        "id": "Wejl1fh6jcmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature selection\n",
        "\n",
        "# Univariate & f_classif\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "selector = SelectKBest(f_classif,k=\"all\")\n",
        "selector.fit(x_pca,y_train)\n",
        "scores = -np.log10(selector.pvalues_)\n",
        "scores /= scores.max()\n",
        "\n",
        "dataframe = pd.DataFrame()\n",
        "dataframe = selector.pvalues_\n",
        "\n",
        "print(len(selector.pvalues_[selector.pvalues_<0.05]))\n",
        "\n",
        "fs = SelectKBest(score_func=f_classif, k=len(selector.pvalues_[selector.pvalues_<0.05]))\n",
        "X_univariate = fs.fit_transform(x_pca, y_train)\n",
        "print(f'result univariate: {X_univariate.shape}')\n",
        "\n",
        "# Elastic net --> is dit wel een goede fs?\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "ENreg = ElasticNet(alpha=1, l1_ratio=0.5).fit(x_pca,y_train)\n",
        "model_ENreg = SelectFromModel(ENreg, prefit=True)\n",
        "x_ENreg = model_ENreg.transform(x_pca)\n",
        "\n",
        "print(f'result Elastic Net: {x_ENreg.shape}')\n",
        "\n",
        "#x_ENreg = pd.DataFrame(x_ENreg)\n"
      ],
      "metadata": {
        "id": "mFaLdfI9i5eP",
        "outputId": "b65acb34-e4a5-44ed-d9d1-408389179454",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "36\n",
            "result univariate: (620, 36)\n",
            "result Elastic Net: (620, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimize hyperparameters of all classifiers:**"
      ],
      "metadata": {
        "id": "cKBOYqkscOwn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSVM"
      ],
      "metadata": {
        "id": "7niOsrJQcUCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import uniform\n",
        "from sklearn.metrics import f1_score, make_scorer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import fbeta_score\n",
        "\n",
        "l_svm = LinearSVC()\n",
        "#logistic = LogisticRegression(solver='saga', tol=1e-2, max_iter=200,\n",
        "                              #random_state=0)\n",
        "distributions = dict(penalty=['l2', 'l1'],\n",
        "                     loss = ['hinge','squared_hinge'],                                       \n",
        "                     dual=[False, False],\n",
        "                     tol = np.geomspace(1e-7, 1e2, num=10),\n",
        "                     C = np.geomspace(1e-5, 1e5, num=11),\n",
        "                     fit_intercept=[True, True],\n",
        "                     intercept_scaling = range(1,10),\n",
        "                     class_weight = ['balanced','balanced'])\n",
        "#y_pred = clf.predict(x_train)\n",
        "#l_svm.fit(x_train, y_train)\n",
        "\n",
        "\n",
        "#y_pred = l_svm.fit(x_train, y_train).predict(x_train)\n",
        "\n",
        "clf = RandomizedSearchCV(l_svm, distributions, random_state=0, error_score=0.0, scoring=make_scorer(fbeta_score, beta=2)) #dual = False als n_samples > n_features \n",
        "search = clf.fit(x_train, y_train)\n",
        "print(search.best_params_)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zLEYc6rhe8Kp",
        "outputId": "dac48367-0f6a-4472-d058-93c9d857e8be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LibLinear][LibLinear][LibLinear][LibLinear]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LibLinear][LibLinear]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LibLinear]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LibLinear]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LibLinear]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LibLinear]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
            "20 fits failed out of a total of 50.\n",
            "The score on these train-test partitions for these parameters will be set to 0.0.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "15 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
            "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
            "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
            "    raise ValueError(\n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/svm/_classes.py\", line 274, in fit\n",
            "    self.coef_, self.intercept_, n_iter_ = _fit_liblinear(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py\", line 1223, in _fit_liblinear\n",
            "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py\", line 1062, in _get_liblinear_solver_type\n",
            "    raise ValueError(\n",
            "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LibLinear]{'verbose': 2, 'tol': 0.0001, 'penalty': 'l1', 'loss': 'squared_hinge', 'intercept_scaling': 2, 'fit_intercept': True, 'dual': False, 'class_weight': 'balanced', 'C': 0.01}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN"
      ],
      "metadata": {
        "id": "sAqdhm-IcXR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "distributions = dict(n_neighbors=range(1,10),\n",
        "                     weights=['uniform','distance', None],\n",
        "                     algorithm=['auto','auto'],\n",
        "                     leaf_size=range(0,100),\n",
        "                     p=[1,2])\n",
        "\n",
        "clf = RandomizedSearchCV(knn, distributions, random_state=0, error_score=0.0, scoring=make_scorer(fbeta_score, beta=2)) \n",
        "search = clf.fit(x_train, y_train)\n",
        "print(search.best_params_)\n",
        "\n"
      ],
      "metadata": {
        "id": "FP5jJm_cYyZZ",
        "outputId": "0fd86dec-233a-427c-c75a-9f586b20db5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'weights': 'distance', 'p': 1, 'n_neighbors': 2, 'leaf_size': 46, 'algorithm': 'auto'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DT"
      ],
      "metadata": {
        "id": "XpPbWSP6cYse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import tree\n",
        "import numpy as np\n",
        "\n",
        "tree = tree.DecisionTreeClassifier()\n",
        "\n",
        "distributions = dict(criterion =['gini','entropy','log_loss'],\n",
        "                     splitter=['best','random'],\n",
        "                     min_samples_split=range(2,10),\n",
        "                     min_samples_leaf=range(1,10),\n",
        "                     min_weight_fraction_leaf = np.linspace(0, 0.5, 25),\n",
        "                     max_features=['sqrt','log2',None],\n",
        "                     class_weight=['balanced','balanced'])\n",
        "\n",
        "clf = RandomizedSearchCV(tree, distributions, random_state=0, error_score=0.0, scoring=make_scorer(fbeta_score, beta=2)) \n",
        "search = clf.fit(x_train, y_train)\n",
        "print(search.best_params_)"
      ],
      "metadata": {
        "id": "hz7U2TsDbQ-4",
        "outputId": "048ba7f6-bf21-4a29-add0-b11f7c04423d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'splitter': 'random', 'min_weight_fraction_leaf': 0.125, 'min_samples_split': 6, 'min_samples_leaf': 8, 'max_features': None, 'criterion': 'gini', 'class_weight': 'balanced'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVC"
      ],
      "metadata": {
        "id": "7JMTufoxh7BH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "\n",
        "svm = svm.SVC()\n",
        "\n",
        "distributions = dict(C = np.geomspace(1e-5, 1e5, num=11),\n",
        "                     kernel=['linear','poly','rbf','sigmoid'],\n",
        "                     degree = range(0,5),\n",
        "                     gamma=['scale','auto'],\n",
        "                     coef0 = range(-5,5),\n",
        "                     shrinking = [True,False],\n",
        "                     probability = [True, False],\n",
        "                     tol = np.geomspace(1e-7, 1e2, num=10),\n",
        "                     cache_size=range(10,400),\n",
        "                     class_weight = ['balanced','balanced'])\n",
        "\n",
        "clf = RandomizedSearchCV(svm, distributions, random_state=0, error_score=0.0, scoring=make_scorer(fbeta_score, beta=2)) #dual = False als n_samples > n_features \n",
        "\n",
        "search = clf.fit(x_train, y_train)\n",
        "print(search.best_params_)"
      ],
      "metadata": {
        "id": "aDQtky5Hh5Yb",
        "outputId": "83327400-5e88-43f7-bede-011f40c4cde8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM]{'verbose': 1, 'tol': 100.0, 'shrinking': False, 'probability': False, 'kernel': 'poly', 'gamma': 'auto', 'degree': 2, 'coef0': 1, 'class_weight': 'balanced', 'cache_size': 150, 'C': 0.01}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RF"
      ],
      "metadata": {
        "id": "xlQBNVTonxSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "RF = RandomForestClassifier()\n",
        "\n",
        "distributions = dict(n_estimators = range(5,1000,5),\n",
        "                     criterion =['gini','entropy','log_loss'],\n",
        "                     min_samples_split=range(2,10),\n",
        "                     min_samples_leaf=range(1,10),\n",
        "                     min_weight_fraction_leaf = np.linspace(0, 0.5, 25),\n",
        "                     max_features=['sqrt','log2',None],\n",
        "                     bootstrap=[True,False],\n",
        "                     oob_score=[True,False],\n",
        "                     warm_start=[True,False],\n",
        "                     class_weight=['balanced','balanced_subsample'])\n",
        "\n",
        "clf = RandomizedSearchCV(RF, distributions, random_state=0, error_score=0.0, scoring=make_scorer(fbeta_score, beta=2)) \n",
        "search = clf.fit(x_train, y_train)\n",
        "print(search.best_params_)"
      ],
      "metadata": {
        "id": "Be41HWQNny5q",
        "outputId": "2ee787df-2ea7-44c4-ad30-59a9d0d80d47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_forest.py:777: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn(\n"
          ]
        }
      ]
    }
  ]
}